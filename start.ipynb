{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"this_python\"] = sys.executable  # get the right python executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.8.18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20191211_181859-zdt5nxpk\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpolished-plasma-1184\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/predictive-analytics-lab/nosinn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/predictive-analytics-lab/nosinn/runs/zdt5nxpk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "/mnt/data1/finn/NoSINN/nosinn/optimisation/train_nosinn.py\n",
      "Namespace(ae_channels=61, ae_enc_dim=35, ae_epochs=60, ae_levels=0, ae_loss=mixed, ae_loss_weight=1, autoencode=True, background=False, base_density=normal, base_density_std=1, batch_norm=False, batch_size=1000, binarize=True, black=True, bn_lag=0, celeba_sens_attr=['Male'], celeba_target_attr=Smiling, coupling_channels=20, coupling_depth=1, data_pcnt=1.0, data_split_seed=888, dataset=adult, disc_channels=256, disc_depth=1, disc_hidden_dims=[256], disc_lr=0.0003, drop_discrete=False, drop_native=True, early_stopping=30, epochs=20, eval_epochs=40, eval_lr=0.001, evaluate=False, factor_splits={}, gamma=0.96, glow=True, gp_weight=0, gpu=0, greyscale=False, idf=False, input_noise=False, kl_weight=0.1, level_depth=1, levels=3, log_freq=50, lr=0.001, nll_weight=1.0, padding=2, path_to_ae=, pred_s_weight=0.003, preliminary_level=False, pretrain=True, pretrain_pcnt=0.4, quant_level=8, recon_detach=True, recon_stability_weight=0, reshape_method=squeeze, results_csv=, resume=None, root=data, rotate_data=False, save_dir=experiments/finn, scale=0.02, scaling=add2_sigmoid, seed=42, shift_data=False, skip_disc_steps=1, spectral_norm=False, super_val=True, supply_s=False, task_mixing_factor=0.0, task_pcnt=0.2, test_batch_size=None, train_on_recon=True, use_wandb=True, vae=False, val_freq=20, warmup_steps=0, weight_decay=1e-06, zs_frac=0.03)\n",
      "Save directory: /mnt/data1/finn/NoSINN/experiments/finn/1576088340.521023\n",
      "1 GPUs available. Using device 'cuda:0'\n",
      "Size of pretrain: 18089, task_train: 9044, task: 9026\n",
      "===> Fitting Auto-encoder to the training data....\n",
      "100%|█████████████████| 1140/1140 [00:20<00:00, 56.17it/s, AE_loss=0.0139308795]\n",
      "zs dim: 1\n",
      "zy dim: 34\n",
      "Number of trainable parameters: 9726\n",
      "[TRN] Epoch 0000 | Duration: 3.59s | Batches/s: 9.516 | Loss NLL: 1.3751 | Loss Adversarial: 0.0045478 | Recon loss: 0 | Validation loss: 1.3705 (1.3796)\n",
      "[TRN] Epoch 0001 | Duration: 3.92s | Batches/s: 11.24 | Loss NLL: 1.3552 | Loss Adversarial: 0.0044372 | Recon loss: 0 | Validation loss: 1.3507 (1.3596)\n",
      "[TRN] Epoch 0002 | Duration: 4.21s | Batches/s: 10.97 | Loss NLL: 1.3293 | Loss Adversarial: 0.0043184 | Recon loss: 0 | Validation loss: 1.3249 (1.3336)\n",
      "[TRN] Epoch 0003 | Duration: 4.03s | Batches/s: 11.28 | Loss NLL: 1.3017 | Loss Adversarial: 0.0042507 | Recon loss: 0 | Validation loss: 1.2974 (1.3059)\n",
      "[TRN] Epoch 0004 | Duration: 4.16s | Batches/s: 11.22 | Loss NLL: 1.275 | Loss Adversarial: 0.0042005 | Recon loss: 0 | Validation loss: 1.2708 (1.2792)\n",
      "[TRN] Epoch 0005 | Duration: 4.03s | Batches/s: 11.02 | Loss NLL: 1.2483 | Loss Adversarial: 0.0041742 | Recon loss: 0 | Validation loss: 1.2441 (1.2525)\n",
      "[TRN] Epoch 0006 | Duration: 4.17s | Batches/s: 11.22 | Loss NLL: 1.2255 | Loss Adversarial: 0.004161 | Recon loss: 0 | Validation loss: 1.2213 (1.2297)\n",
      "[TRN] Epoch 0007 | Duration: 4.32s | Batches/s: 10.98 | Loss NLL: 1.202 | Loss Adversarial: 0.0041496 | Recon loss: 0 | Validation loss: 1.1978 (1.2061)\n",
      "[TRN] Epoch 0008 | Duration: 3.91s | Batches/s: 11.7 | Loss NLL: 1.1803 | Loss Adversarial: 0.0041351 | Recon loss: 0 | Validation loss: 1.1761 (1.1844)\n",
      "[TRN] Epoch 0009 | Duration: 4.04s | Batches/s: 11.21 | Loss NLL: 1.1608 | Loss Adversarial: 0.0041256 | Recon loss: 0 | Validation loss: 1.1567 (1.1649)\n",
      "[TRN] Epoch 0010 | Duration: 4.3s | Batches/s: 10.96 | Loss NLL: 1.1413 | Loss Adversarial: 0.0041123 | Recon loss: 0 | Validation loss: 1.1372 (1.1454)\n",
      "[TRN] Epoch 0011 | Duration: 4.11s | Batches/s: 11.26 | Loss NLL: 1.1226 | Loss Adversarial: 0.0041043 | Recon loss: 0 | Validation loss: 1.1185 (1.1267)\n",
      "[TRN] Epoch 0012 | Duration: 4.21s | Batches/s: 11.21 | Loss NLL: 1.1048 | Loss Adversarial: 0.0040982 | Recon loss: 0 | Validation loss: 1.1007 (1.1089)\n",
      "[TRN] Epoch 0013 | Duration: 3.98s | Batches/s: 10.95 | Loss NLL: 1.0891 | Loss Adversarial: 0.0040882 | Recon loss: 0 | Validation loss: 1.085 (1.0932)\n",
      "[TRN] Epoch 0014 | Duration: 4.06s | Batches/s: 11.24 | Loss NLL: 1.074 | Loss Adversarial: 0.0040774 | Recon loss: 0 | Validation loss: 1.0699 (1.0781)\n",
      "[TRN] Epoch 0015 | Duration: 3.99s | Batches/s: 11.01 | Loss NLL: 1.0574 | Loss Adversarial: 0.0040657 | Recon loss: 0 | Validation loss: 1.0533 (1.0615)\n",
      "[TRN] Epoch 0016 | Duration: 3.97s | Batches/s: 11.63 | Loss NLL: 1.0438 | Loss Adversarial: 0.0040675 | Recon loss: 0 | Validation loss: 1.0398 (1.0479)\n",
      "[TRN] Epoch 0017 | Duration: 4.27s | Batches/s: 11.2 | Loss NLL: 1.0297 | Loss Adversarial: 0.0040552 | Recon loss: 0 | Validation loss: 1.0257 (1.0338)\n",
      "[TRN] Epoch 0018 | Duration: 4.51s | Batches/s: 11.06 | Loss NLL: 1.0169 | Loss Adversarial: 0.0040506 | Recon loss: 0 | Validation loss: 1.0128 (1.0209)\n",
      "[TRN] Epoch 0019 | Duration: 4.27s | Batches/s: 11.24 | Loss NLL: 1.002 | Loss Adversarial: 0.004039 | Recon loss: 0 | Validation loss: 0.99795 (1.006)\n",
      "Training has finished.\n",
      "Encoding task dataset...\n",
      "Encoding task train dataset...\n",
      "\n",
      "Computing metrics...\n",
      "/mnt/data1/finn/EthicML/ethicml/metrics/tpr.py:18: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return t_pos / (t_pos + f_neg)\n",
      "/mnt/data1/finn/EthicML/ethicml/metrics/tnr.py:18: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return t_neg / (t_neg + f_pos)\n",
      "Results for xy_y_on_recons:\n",
      "\t\tAccuracy: 0.8272\n",
      "\t\tTPR: 0.6255\n",
      "\t\tTNR: 0.9749\n",
      "\t\tPPV: 0.9480\n",
      "\t\tNMI preds and y: 0.3852\n",
      "\t\tNMI preds and s: 0.3852\n",
      "\t\tprob_pos_sex_Male_1.0: 0.6255\n",
      "\t\tprob_pos_sex_Male_0.0: 0.0251\n",
      "\t\tprob_pos_sex_Male_0.0-sex_Male_1.0: 0.6004\n",
      "\t\tprob_pos_sex_Male_0.0/sex_Male_1.0: 0.0402\n",
      "\t\tTPR_sex_Male_1.0: 0.6255\n",
      "\t\tTPR_sex_Male_0.0: nan\n",
      "\t\tTPR_sex_Male_0.0-sex_Male_1.0: nan\n",
      "\t\tTPR_sex_Male_0.0/sex_Male_1.0: nan\n",
      "\t\tTNR_sex_Male_1.0: nan\n",
      "\t\tTNR_sex_Male_0.0: 0.9749\n",
      "\t\tTNR_sex_Male_0.0-sex_Male_1.0: nan\n",
      "\t\tTNR_sex_Male_0.0/sex_Male_1.0: 1.0000\n",
      "\t\tPPV_sex_Male_1.0: 1.0000\n",
      "\t\tPPV_sex_Male_0.0: 0.0000\n",
      "\t\tPPV_sex_Male_0.0-sex_Male_1.0: 1.0000\n",
      "\t\tPPV_sex_Male_0.0/sex_Male_1.0: 0.0000\n",
      "\t\tNMI preds and y_sex_Male_1.0: 1.0000\n",
      "\t\tNMI preds and y_sex_Male_0.0: 7.7969\n",
      "\t\tNMI preds and y_sex_Male_0.0-sex_Male_1.0: 6.7969\n",
      "\t\tNMI preds and y_sex_Male_0.0/sex_Male_1.0: 0.1283\n",
      "\t\tNMI preds and s_sex_Male_1.0: 1.0000\n",
      "\t\tNMI preds and s_sex_Male_0.0: 7.7969\n",
      "\t\tNMI preds and s_sex_Male_0.0-sex_Male_1.0: 6.7969\n",
      "\t\tNMI preds and s_sex_Male_0.0/sex_Male_1.0: 0.1283\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 20322\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                         _step 378\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      Loss NLL 0.9951852560043335\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      _runtime 105.74681353569031\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               Validation loss 0.9911405444145203\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              Loss Adversarial 0.004044689703732729\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    Recon loss 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    _timestamp 1576088444.9395995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 8 W&B file(s) and 40 media file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced polished-plasma-1184: https://app.wandb.ai/predictive-analytics-lab/nosinn/runs/zdt5nxpk\n"
     ]
    }
   ],
   "source": [
    "!$this_python -u start_nosinn.py \\\n",
    "--ae-channels 61 \\\n",
    "--ae-enc-dim 35 \\\n",
    "--ae-epochs 60 \\\n",
    "--ae-levels 0 \\\n",
    "--ae-loss mixed \\\n",
    "--autoencode True \\\n",
    "--batch-norm False \\\n",
    "--batch-size 1000 \\\n",
    "--coupling-channels 20 \\\n",
    "--coupling-depth 1 \\\n",
    "--dataset adult \\\n",
    "--disc-hidden-dims 256 \\\n",
    "--disc-lr 3e-4 \\\n",
    "--drop-native True \\\n",
    "--epochs 20 \\\n",
    "--gamma 0.96 \\\n",
    "--glow True \\\n",
    "--input-noise False \\\n",
    "--level-depth 1 \\\n",
    "--lr 1e-3 \\\n",
    "--nll-weight 1 \\\n",
    "--pred-s-weight 3e-3 \\\n",
    "--scaling add2_sigmoid \\\n",
    "--super-val True \\\n",
    "--task-mixing-factor 0.0 \\\n",
    "--train-on-recon True \\\n",
    "--vae False \\\n",
    "--val-freq 20 \\\n",
    "--weight-decay 1e-6 \\\n",
    "--zs-frac 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (Py 3.7)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
